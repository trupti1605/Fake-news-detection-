{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Fake news detection "}, {"metadata": {}, "cell_type": "markdown", "source": "Importing libraries "}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nfrom time import time\nimport string\n#import itertools\nfrom pprint import pprint\n\nfrom nltk import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n#from gensim import models\n#from gensim.models import word2vec,doc2vec", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Data Exploration"}, {"metadata": {}, "cell_type": "code", "source": "df = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/fake_or_real_news.csv\")\nprint(df.shape)", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "(6335, 4)\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df.head()", "execution_count": 22, "outputs": [{"output_type": "execute_result", "execution_count": 22, "data": {"text/plain": "   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary\u2019s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n3  \u2014 Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n4  It's primary day in New York and front-runners...  REAL  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8476</td>\n      <td>You Can Smell Hillary\u2019s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10294</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3608</td>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>REAL</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10142</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>\u2014 Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>875</td>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>REAL</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Extracting the training data"}, {"metadata": {}, "cell_type": "code", "source": "y = df.label \ndf.drop(\"label\", axis=1) \nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=42)", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(X_train.shape)\nprint(type(X_train))\nprint(X_train.head())\n\n\nprint(X_test.shape)\nprint(type(X_test))\nprint(X_test.head())", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "(4244,)\n<class 'pandas.core.series.Series'>\n5593    The next president is most likely to face an i...\n6067    Following news of FBI Director James Comey\u2019s d...\n3026    Interviews A protester throws a glass bottle c...\n4385    Will it be representative government or thugoc...\n120     It is increasingly apparent that the U.S. war ...\nName: text, dtype: object\n(2091,)\n<class 'pandas.core.series.Series'>\n1357    Will Trump pull a Brexit times ten? What would...\n2080    Clintons Are Under Multiple FBI Investigations...\n2718    Dispatches from Eric Zuesse This piece is cros...\n812     Print \\n[Ed. \u2013 Every now and then the facade c...\n4886    Nanny In Jail After Force Feeding Baby To Deat...\nName: text, dtype: object\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Let\u2019s initialize a TfidfVectorizer with stop words from the English language and a maximum document frequency of 0.7 (terms with a higher document frequency will be discarded). Stop words are the most common words in a language that are to be filtered out before processing the natural language data. And a TfidfVectorizer turns a collection of raw documents into a matrix of TF-IDF features."}, {"metadata": {}, "cell_type": "code", "source": "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) \ntfidf_train = tfidf_vectorizer.fit_transform(X_train) \ntfidf_test = tfidf_vectorizer.transform(X_test)", "execution_count": 25, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "print(tfidf_train.shape)\nprint(tfidf_test.shape)", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "(4244, 56801)\n(2091, 56801)\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "print(tfidf_vectorizer.get_feature_names()[-10:])", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "['\u05e9\u05ea\u05d9', '\u05ea\u05d0\u05de\u05e6\u05e0\u05d4', '\u05ea\u05d5\u05e6\u05d0\u05d4', '\u05ea\u05d7\u05dc', '\u05ea\u05d9\u05d9\u05e8\u05d5\u05ea', '\u05ea\u05e0\u05d5\u05ea\u05e7', '\u05ea\u05e2\u05d5\u05d3\u05ea', '\u05ea\u05ea\u05e8\u05db\u05d6', '\u0627\u0644\u0642\u0627\u062f\u0645\u0648\u0646', '\u0639\u0631\u0628\u064a']\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())", "execution_count": 28, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "count_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)", "execution_count": 29, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "count_vectorizer.get_feature_names()[:10]", "execution_count": 30, "outputs": [{"output_type": "execute_result", "execution_count": 30, "data": {"text/plain": "['00',\n '000',\n '0000',\n '000000031',\n '00000031',\n '0001',\n '000billion',\n '000ft',\n '000km',\n '001']"}, "metadata": {}}]}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "tfidf_df.head()", "execution_count": 31, "outputs": [{"output_type": "execute_result", "execution_count": 31, "data": {"text/plain": "    00  000  0000  000000031  00000031  0001  000billion  000ft  000km  001  \\\n0  0.0  0.0   0.0        0.0       0.0   0.0         0.0    0.0    0.0  0.0   \n1  0.0  0.0   0.0        0.0       0.0   0.0         0.0    0.0    0.0  0.0   \n2  0.0  0.0   0.0        0.0       0.0   0.0         0.0    0.0    0.0  0.0   \n3  0.0  0.0   0.0        0.0       0.0   0.0         0.0    0.0    0.0  0.0   \n4  0.0  0.0   0.0        0.0       0.0   0.0         0.0    0.0    0.0  0.0   \n\n   ...  \u05e9\u05ea\u05d9  \u05ea\u05d0\u05de\u05e6\u05e0\u05d4  \u05ea\u05d5\u05e6\u05d0\u05d4  \u05ea\u05d7\u05dc  \u05ea\u05d9\u05d9\u05e8\u05d5\u05ea  \u05ea\u05e0\u05d5\u05ea\u05e7  \u05ea\u05e2\u05d5\u05d3\u05ea  \u05ea\u05ea\u05e8\u05db\u05d6  \u0627\u0644\u0642\u0627\u062f\u0645\u0648\u0646  \u0639\u0631\u0628\u064a  \n0  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0       0.0   0.0  \n1  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0       0.0   0.0  \n2  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0       0.0   0.0  \n3  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0       0.0   0.0  \n4  ...  0.0     0.0    0.0  0.0     0.0    0.0    0.0    0.0       0.0   0.0  \n\n[5 rows x 56801 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0000</th>\n      <th>000000031</th>\n      <th>00000031</th>\n      <th>0001</th>\n      <th>000billion</th>\n      <th>000ft</th>\n      <th>000km</th>\n      <th>001</th>\n      <th>...</th>\n      <th>\u05e9\u05ea\u05d9</th>\n      <th>\u05ea\u05d0\u05de\u05e6\u05e0\u05d4</th>\n      <th>\u05ea\u05d5\u05e6\u05d0\u05d4</th>\n      <th>\u05ea\u05d7\u05dc</th>\n      <th>\u05ea\u05d9\u05d9\u05e8\u05d5\u05ea</th>\n      <th>\u05ea\u05e0\u05d5\u05ea\u05e7</th>\n      <th>\u05ea\u05e2\u05d5\u05d3\u05ea</th>\n      <th>\u05ea\u05ea\u05e8\u05db\u05d6</th>\n      <th>\u0627\u0644\u0642\u0627\u062f\u0645\u0648\u0646</th>\n      <th>\u0639\u0631\u0628\u064a</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 56801 columns</p>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Comparing Models"}, {"metadata": {}, "cell_type": "code", "source": "import sklearn.metrics as metrics", "execution_count": 32, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def classify_and_fit(clf, X_train, y_train, X_test, y_test, class_labels = ['FAKE', 'REAL']):\n    print(\"Classifier : \", clf )\n    \n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    score = metrics.accuracy_score(y_test, pred)\n    \n    print(\"Accuracy:   %0.3f\" % score)\n\n    print(\"\\nConfusion Matrix :\")\n    #print(pd.crosstab(y_test, pred, rownames=['True'], colnames=['Predicted'], margins=True))\n    cm = metrics.confusion_matrix(y_test, pred, labels=class_labels)\n    print(cm)\n    \n    print(\"\\nReport :\")    \n    print(classification_report(y_test, pred, target_names=class_labels))\n    \n    \n    return clf", "execution_count": 33, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "clf = MultinomialNB() \nclassify_and_fit(clf, tfidf_train, y_train, tfidf_test, y_test)", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "Classifier :  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nAccuracy:   0.815\n\nConfusion Matrix :\n[[ 704  367]\n [  19 1001]]\n\nReport :\n              precision    recall  f1-score   support\n\n        FAKE       0.97      0.66      0.78      1071\n        REAL       0.73      0.98      0.84      1020\n\n   micro avg       0.82      0.82      0.82      2091\n   macro avg       0.85      0.82      0.81      2091\nweighted avg       0.86      0.82      0.81      2091\n\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 34, "data": {"text/plain": "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Testing Linear Models"}, {"metadata": {}, "cell_type": "code", "source": "linear_clf = PassiveAggressiveClassifier(n_iter=50)\nclassify_and_fit(linear_clf, tfidf_train, y_train, tfidf_test, y_test)", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "Classifier :  PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n              early_stopping=False, fit_intercept=True, loss='hinge',\n              max_iter=None, n_iter=50, n_iter_no_change=5, n_jobs=None,\n              random_state=None, shuffle=True, tol=None,\n              validation_fraction=0.1, verbose=0, warm_start=False)\n", "name": "stdout"}, {"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n  DeprecationWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n  DeprecationWarning)\n", "name": "stderr"}, {"output_type": "stream", "text": "Accuracy:   0.939\n\nConfusion Matrix :\n[[1011   60]\n [  67  953]]\n\nReport :\n              precision    recall  f1-score   support\n\n        FAKE       0.94      0.94      0.94      1071\n        REAL       0.94      0.93      0.94      1020\n\n   micro avg       0.94      0.94      0.94      2091\n   macro avg       0.94      0.94      0.94      2091\nweighted avg       0.94      0.94      0.94      2091\n\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 35, "data": {"text/plain": "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n              early_stopping=False, fit_intercept=True, loss='hinge',\n              max_iter=None, n_iter=50, n_iter_no_change=5, n_jobs=None,\n              random_state=None, shuffle=True, tol=None,\n              validation_fraction=0.1, verbose=0, warm_start=False)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Count versus TF-IDF Features\n"}, {"metadata": {}, "cell_type": "code", "source": "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n    class_labels = classifier.classes_\n    feature_names = vectorizer.get_feature_names()\n    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n    for coef, feat in topn_class1:\n        print(class_labels[0], coef, feat)\n    print()\n    for coef, feat in reversed(topn_class2):\n        print(class_labels[1], coef, feat)\n\n\nmost_informative_feature_for_binary_classification(tfidf_vectorizer, linear_clf, n=30)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "FAKE -5.019620107823401 october\nFAKE -4.991404304543739 2016\nFAKE -4.163823192806515 hillary\nFAKE -2.9399579024910416 share\nFAKE -2.93190002661481 article\nFAKE -2.7568604102431573 election\nFAKE -2.75630523788773 november\nFAKE -2.523696838421442 snip\nFAKE -2.4593226921995712 podesta\nFAKE -2.3871970470868282 corporate\nFAKE -2.341820614734989 source\nFAKE -2.312077564867841 print\nFAKE -2.304100745419877 email\nFAKE -2.2006567342529175 mosul\nFAKE -2.028562280763503 healthcare\nFAKE -1.9385052220996728 wikileaks\nFAKE -1.871761901124584 donald\nFAKE -1.853885163445138 com\nFAKE -1.8524514733191586 establishment\nFAKE -1.8522099260893314 just\nFAKE -1.8515316791872922 stated\nFAKE -1.792994089124951 video\nFAKE -1.7692811344292134 entire\nFAKE -1.7170560749456338 oct\nFAKE -1.653156253415706 advertisement\nFAKE -1.6521635335266147 photo\nFAKE -1.645439054367439 uk\nFAKE -1.632347592035885 demand\nFAKE -1.62103009800192 ___\nFAKE -1.5942594624364452 pay\n\nREAL 5.0043602199944415 said\nREAL 3.0937745288610023 says\nREAL 2.593593339835453 cruz\nREAL 2.417406250438713 conservative\nREAL 2.4127653270629494 candidates\nREAL 2.3772832337391754 convention\nREAL 2.2870308256246927 debate\nREAL 2.24985190154938 tuesday\nREAL 2.246679967831185 marriage\nREAL 2.223012182420435 gop\nREAL 2.1984664062455517 friday\nREAL 2.0789387947352926 state\nREAL 2.0148943813247286 conservatives\nREAL 1.9619448120980196 nomination\nREAL 1.9578994556859042 fox\nREAL 1.9562391663517826 rush\nREAL 1.9523041668412071 monday\nREAL 1.8693066069500754 continue\nREAL 1.828699410922609 attack\nREAL 1.8261280136548874 2012\nREAL 1.8025616704487435 tapping\nREAL 1.7999117301687488 recounts\nREAL 1.7520169126869705 islamic\nREAL 1.7484113177848823 message\nREAL 1.7037595926819613 wni9lmsppr\nREAL 1.6400797064077612 parties\nREAL 1.6338048717979061 income\nREAL 1.620220493725309 shooting\nREAL 1.6030726874270311 jobs\nREAL 1.5928954519568115 baltimore\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}